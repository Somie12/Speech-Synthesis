{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fNzvRI_jDhezU7Wd7anjx6dzQBD0DxkI",
      "authorship_tag": "ABX9TyM9SPl+bCOHDT6TnX8rJqNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somie12/Speech-Synthesis/blob/main/TTS_Audio_Generation_and_PESQ_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZRmtt9-Dy7G",
        "outputId": "8e51560a-5543-4d8d-f608-cabf395c953f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,704 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,639 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,531 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,941 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,664 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,799 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,319 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,661 kB]\n",
            "Fetched 28.9 MB in 4s (6,495 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  espeak-data espeak-ng-data libespeak-ng1 libespeak1 libpcaudio0 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data espeak-ng espeak-ng-data libespeak-ng1 libespeak1 libpcaudio0 libportaudio2\n",
            "  libsonic0\n",
            "0 upgraded, 9 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 5,897 kB of archives.\n",
            "After this operation, 15.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak-data amd64 1.48.15+dfsg-3 [1,085 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libespeak1 amd64 1.48.15+dfsg-3 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak amd64 1.48.15+dfsg-3 [64.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3,956 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Fetched 5,897 kB in 2s (2,741 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 124935 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../1-libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../2-espeak-data_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../3-libespeak1_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../4-espeak_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "Preparing to unpack .../5-libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../6-espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../7-libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../8-espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up espeak (1.48.15+dfsg-3) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.0.12)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.13.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from TTS) (2.5.1+cu124)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.10.2.post1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.6.1)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (7.5.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.67.1)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.11.12)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (24.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.1.0)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting umap-learn>=0.5.1 (from TTS)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas<2.0,>=1.4 (from TTS)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.10.0)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from TTS) (3.9.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.8.1)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.48.3)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.11/dist-packages (from spacy[ja]>=3->TTS) (3.7.5)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.26.4)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.61.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.17.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.18.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (10.6.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57.0->TTS) (0.44.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0,>=1.4->TTS) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->TTS) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.0->TTS) (1.17.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.15.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.5.0)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiDict_core-20250129-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.17.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.5.2)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (3.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->TTS) (4.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (4.25.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl (937 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiDict_core-20250129-py3-none-any.whl (72.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75788 sha256=c9cb50e2cf545f10be8f2ae2e338298d7a017fd37a1c4f37a00392284a6a4dd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/a0/bc/4dacab52579ab464cffafbe7a8e3792dd36ad9ac288b264843\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=07bb6446e0ce35284ebf2325b1f2a1a1fc3fd6e4b27600cf658e6b38747d108e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/a4/88/480018a664e58ca7ce6708759193ee51b017b3b72aa3df8a85\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=9ea175878c7b404c54f3f8d6b44524cd8ac34b68f3c5b2184c7795e59344976f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/b9/e3/4145416693824818c0b931988a692676ecd4bbf2ea41d1eedd\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=dc9d6a53947848a2459657d53d80e8a538650f1861c067762acf3f39fe838d61\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=e57467756ed22d3b2d485a186ed315c7495732b2ccc634407e7a515418e2c8c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/10/89/a5908dd7a9a032229684b7679396785e19f816667f788087fb\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498313 sha256=23195ccc751b1fdbb94b2a26ed168f78c783a98094fffca4677b4f7a5a1dac2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/fa/df/5fdf5d3cc26ba859b8698a1f28581d1a6aa081edc6df9847ab\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326857 sha256=63bea63e8417e8704790f661537c052657fe7e29dc302caa5fbdeeb2d194e8e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/30/52/dc5cd222b4bbde285838fed1f96636e96f85cd75493e79a978\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173928 sha256=1bb4e34db39e88b924c8403b800b1c6859824528eab3712268a6427649da2bcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/eb/59/30b5d15e56347e595f613036cbea0f807ad9621c75cd75d912\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=d1c86ba1865c4d5ed04114708121516a094c05e8a61e21295e272a4896ae76fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/e7/a0/7c416a3eeaa94ca71bf7bcbc6289cced2263d8ba35e82444bb\n",
            "Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, python-crfsuite, pysbd, pypinyin, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, g2pkk, dateparser, pynndescent, nvidia-cusolver-cu12, gruut, umap-learn, trainer, encodec, TTS\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "nx-cugraph-cu12 24.12.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
            "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 networkx-2.8.8 num2words-0.5.14 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 sudachidict-core-20250129 sudachipy-0.6.10 trainer-0.0.36 umap-learn-0.5.7 unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y espeak-ng espeak ffmpeg\n",
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from TTS.api import TTS\n",
        "import os\n",
        "import re\n",
        "\n",
        "# This code generates audio in Chinese using the XTTS model from the transcripts and saves it in the file.\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/selected audio 3/Transcriptions/chinese transcriptions\"\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio\"\n",
        "\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
        "\n",
        "txt_files = sorted(\n",
        "    [f for f in os.listdir(INPUT_FOLDER) if f.endswith(\".txt\")],\n",
        "    key=lambda x: int(re.search(r'\\d+', x).group())\n",
        ")\n",
        "\n",
        "for filename in txt_files:\n",
        "    transcript_path = os.path.join(INPUT_FOLDER, filename)\n",
        "\n",
        "    with open(transcript_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read().strip()\n",
        "\n",
        "    file_number = re.search(r'\\d+', filename).group()\n",
        "    audio_filename = f\"{file_number}.wav\"\n",
        "    audio_path = os.path.join(OUTPUT_FOLDER, audio_filename)\n",
        "\n",
        "    print(f\"Generating audio for: {filename}\")\n",
        "    speaker_audio = f\"/content/drive/MyDrive/selected audio 3/chinese/101.wav\"\n",
        "\n",
        "    tts.tts_to_file(text=text, speaker_wav=speaker_audio, language=\"zh\", file_path=audio_path)\n",
        "\n",
        "    print(f\"Saved: {audio_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kELn-TzYD78N",
        "outputId": "854c9010-48e9-4b43-ab6b-c056f97c5363"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > You must confirm the following:\n",
            " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
            " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n",
            " | | > y\n",
            " > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1.87G/1.87G [00:30<00:00, 65.7MiB/s]\n",
            "100%|██████████| 1.87G/1.87G [00:30<00:00, 61.4MiB/s]\n",
            "100%|██████████| 4.37k/4.37k [00:00<00:00, 12.2kiB/s]\n",
            " 55%|█████▍    | 198k/361k [00:00<00:00, 532kiB/s] \n",
            "100%|██████████| 361k/361k [00:00<00:00, 470kiB/s]\n",
            "100%|██████████| 32.0/32.0 [00:00<00:00, 85.1iB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Model's license - CPML\n",
            " > Check https://coqui.ai/cpml.txt for more info.\n",
            " > Using model: xtts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.speakers = torch.load(speaker_file_path)\n",
            "/usr/local/lib/python3.11/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=map_location, **kwargs)\n",
            "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating audio for: 101.txt\n",
            " > Text splitted to sentences.\n",
            "['但是在我走路还一瘸一拐还瘸的时候就很明显一看就是个瘸子']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Processing time: 6.25743556022644\n",
            " > Real-time factor: 1.058751182496877\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/101.wav\n",
            "Generating audio for: 102.txt\n",
            " > Text splitted to sentences.\n",
            "['我就又开始滑板了因为我太爱滑板了之所以我爱滑板 我迷恋滑板']\n",
            " > Processing time: 3.2048232555389404\n",
            " > Real-time factor: 0.423334328480744\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/102.wav\n",
            "Generating audio for: 103.txt\n",
            " > Text splitted to sentences.\n",
            "['是因为滑板的魅力是因为滑板难滑板太难了滑板告诉我什么是勇往直前什么是知难而进什么是执着']\n",
            " > Processing time: 4.549943208694458\n",
            " > Real-time factor: 0.4227324536157251\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/103.wav\n",
            "Generating audio for: 104.txt\n",
            " > Text splitted to sentences.\n",
            "['我得洛杉磯见屁卧了见泡手追个死了我见人家的时候我是有点人腿是迈不开了我说啥我说啥我迈不开腿了真的我丢滑板人']\n",
            " > Processing time: 4.985625505447388\n",
            " > Real-time factor: 0.39322469808817495\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/104.wav\n",
            "Generating audio for: 105.txt\n",
            " > Text splitted to sentences.\n",
            "['而我们中国人干一个月才能买一套花瓣这是什么比例啊许诗说你这么说只能证明你不爱国']\n",
            " > Processing time: 3.936950922012329\n",
            " > Real-time factor: 0.47224392805276705\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/105.wav\n",
            "Generating audio for: 106.txt\n",
            " > Text splitted to sentences.\n",
            "['我兄弟我这不是爱国不爱国我的意思呀我们是我们玩滑板的买不起滑板你听没听懂啊']\n",
            " > Processing time: 3.740791082382202\n",
            " > Real-time factor: 0.39386337462051896\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/106.wav\n",
            "Generating audio for: 107.txt\n",
            " > Text splitted to sentences.\n",
            "['真的 我跟你说个真事啊当笑话听就行了我当时我听完我笑不出来也挺好笑但是我说啥笑不出来在我们行台啊一个小孩想买滑板就给他妈在那儿商量的他妈说 走小孩给你卖个车来这小子就说']\n",
            "[!] Warning: The text length exceeds the character limit of 82 for language 'zh', this might cause truncated audio.\n",
            " > Processing time: 7.436622858047485\n",
            " > Real-time factor: 0.46447296062754095\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/107.wav\n",
            "Generating audio for: 108.txt\n",
            " > Text splitted to sentences.\n",
            "['人家整过这个没有啊人家一个小孩最后都成了天皇巨星了在滑板滑的你说咱这玩滑板中国的滑板爱好者们呢咱能不能以后别互相骂了行不行啊']\n",
            " > Processing time: 5.631568193435669\n",
            " > Real-time factor: 0.40386142043911805\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/108.wav\n",
            "Generating audio for: 109.txt\n",
            " > Text splitted to sentences.\n",
            "['人民币不值钱,滑板太贵在中国呀,一套滑板最便宜的399除了儿童版,是吧399是什么概念呢正常组装的滑板怎么得1000块钱一套']\n",
            " > Processing time: 6.47388768196106\n",
            " > Real-time factor: 0.47984867755083016\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/109.wav\n",
            "Generating audio for: 110.txt\n",
            " > Text splitted to sentences.\n",
            "['他去餐馆打了两天工赚了400块钱就是说呀美国人两天的工资能卖三套滑板']\n",
            " > Processing time: 4.361145257949829\n",
            " > Real-time factor: 0.5173627708196701\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/110.wav\n",
            "Generating audio for: 111.txt\n",
            " > Text splitted to sentences.\n",
            "['下雨 人家去美国旅游去了 拿个滑板没事儿凹了几下你搞对象吗 人家下雨 媳妇儿都有孩子了 还去滑雪滑板呢 这只能说啥呀']\n",
            " > Processing time: 5.10435938835144\n",
            " > Real-time factor: 0.510593400745578\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/111.wav\n",
            "Generating audio for: 112.txt\n",
            " > Text splitted to sentences.\n",
            "['你那些理由嘛都是扯犊子全是用不着的 是不是我在西安呢 生活了半年在西安呀 交到很多滑板的朋友']\n",
            " > Processing time: 4.4991044998168945\n",
            " > Real-time factor: 0.3958077490462916\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/112.wav\n",
            "Generating audio for: 113.txt\n",
            " > Text splitted to sentences.\n",
            "['小万啊,阿勇啊,张婷宇,六六等等的丫头阿勇啊,人家温州人,出生在新疆就是因为新疆玩滑板的少,没人滑']\n",
            " > Processing time: 4.699888467788696\n",
            " > Real-time factor: 0.4563541037603958\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/113.wav\n",
            "Generating audio for: 114.txt\n",
            " > Text splitted to sentences.\n",
            "['人家阿勇为了花碗移民了,移民长安去了,移民西安了,是吧一开始阿勇啊,自己开店,白天啊,盯一天店']\n",
            " > Processing time: 3.793578863143921\n",
            " > Real-time factor: 0.399421336295379\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/114.wav\n",
            "Generating audio for: 115.txt\n",
            " > Text splitted to sentences.\n",
            "['白天在店里忙活一天了晚上啊自己跑着浮雕廣場滑板去了這是什麼樣的精神後來了人家阿勇給別人打工去了給人家打工去了一站站一天']\n",
            " > Processing time: 4.330082416534424\n",
            " > Real-time factor: 0.3967418941750218\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/115.wav\n",
            "Generating audio for: 116.txt\n",
            " > Text splitted to sentences.\n",
            "['站那脚丫头马了,累一天了,累成狗了,真的累那么啥都不是啥了,最后下班了还得滑一会儿板你看看人家啊,这才是一个真正的滑板人对生活,对滑板,对自己的态度']\n",
            " > Processing time: 5.547416925430298\n",
            " > Real-time factor: 0.44820507418412553\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/116.wav\n",
            "Generating audio for: 117.txt\n",
            " > Text splitted to sentences.\n",
            "['你看看你,你看看,這個啦,那啦,挑開你啥都不是啥,聽懂,聽沒聽懂那些天天用嘴滑板的那些人']\n",
            " > Processing time: 3.2112045288085938\n",
            " > Real-time factor: 0.4025690204006498\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/117.wav\n",
            "Generating audio for: 118.txt\n",
            " > Text splitted to sentences.\n",
            "['什么是滑板啊']\n",
            " > Processing time: 0.6711616516113281\n",
            " > Real-time factor: 0.3611654241026402\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/118.wav\n",
            "Generating audio for: 119.txt\n",
            " > Text splitted to sentences.\n",
            "['人家张婷宇啊,为了自己的理想,都干到非洲精八不韦去了都干到非洲了,人家还天天玩滑板呢,是不是滑板呢,是多么高兴的一件事周末了,下班了,放学了']\n",
            " > Processing time: 6.487732648849487\n",
            " > Real-time factor: 0.45540195368490294\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/119.wav\n",
            "Generating audio for: 120.txt\n",
            " > Text splitted to sentences.\n",
            "['年轻人聚在一起多好啊但是总有那个别那几个那要闹独立的']\n",
            " > Processing time: 2.5891776084899902\n",
            " > Real-time factor: 0.3946698807322495\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/120.wav\n",
            "Generating audio for: 121.txt\n",
            " > Text splitted to sentences.\n",
            "['老互相传闲话有什么用啊既然大家呀都是因为玩滑板走到一起都在一起玩了这了那了你掉链了不掉链了啊']\n",
            " > Processing time: 5.544579982757568\n",
            " > Real-time factor: 0.5684937347472491\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/121.wav\n",
            "Generating audio for: 122.txt\n",
            " > Text splitted to sentences.\n",
            "['中国所有玩滑板的人听完这个视频周末都一块吃顿饭大家能在一起这都是朋友都是哥们整不好最后都成拜把兄弟了以后到社会上整不定不住能互相帮助啥']\n",
            " > Processing time: 5.2653443813323975\n",
            " > Real-time factor: 0.4056293099403941\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/122.wav\n",
            "Generating audio for: 123.txt\n",
            " > Text splitted to sentences.\n",
            "['你装大哥去了你天天顺着个那么绝大个的你能干点啥呀啊你老笑话人瞧不起人']\n",
            " > Processing time: 3.8458096981048584\n",
            " > Real-time factor: 0.46131138394993104\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/123.wav\n",
            "Generating audio for: 124.txt\n",
            " > Text splitted to sentences.\n",
            "['你看看人家真正的职业滑手说没说过这些话你看看生活中网上']\n",
            " > Processing time: 2.4557597637176514\n",
            " > Real-time factor: 0.40208434410994276\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/124.wav\n",
            "Generating audio for: 125.txt\n",
            " > Text splitted to sentences.\n",
            "['什麼樣的人經常說這些不要臉的話啊說真的你說全亞洲全中國全世界的華人']\n",
            " > Processing time: 3.497708320617676\n",
            " > Real-time factor: 0.4027303266230457\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/125.wav\n",
            "Generating audio for: 126.txt\n",
            " > Text splitted to sentences.\n",
            "['你拔河得好咱一起上大街街道得好干什么都行是不是但是咱这个团队要互相鼓励你老互相损互相骂你说咱能干成点啥啊']\n",
            " > Processing time: 5.208145618438721\n",
            " > Real-time factor: 0.4751721734796996\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/126.wav\n",
            "Generating audio for: 127.txt\n",
            " > Text splitted to sentences.\n",
            "['你说咱能干成啥吧 是不是老互相看不起那你说能干点啥我跟你说这样啥都干不好滑板呀 咱比的是啥呀']\n",
            " > Processing time: 4.094615459442139\n",
            " > Real-time factor: 0.39938367400691466\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/127.wav\n",
            "Generating audio for: 128.txt\n",
            " > Text splitted to sentences.\n",
            "['都低調成什麼樣了啊你看看你炸大呼呼的跟跳跳糖似的你說你能幹啥吧啊一個人呢最大的遺憾就是沒有愛好在這一生中他沒有愛好那麼確實很遺憾']\n",
            " > Processing time: 6.337172269821167\n",
            " > Real-time factor: 0.4433881065313142\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/128.wav\n",
            "Generating audio for: 129.txt\n",
            " > Text splitted to sentences.\n",
            "['但是一个人呢最大最大的遗憾就是他放弃了他的爱好一 你没有得绝症二 你还活着三']\n",
            " > Processing time: 3.0219171047210693\n",
            " > Real-time factor: 0.39613616569425697\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/129.wav\n",
            "Generating audio for: 130.txt\n",
            " > Text splitted to sentences.\n",
            "['那老公整的虛的最后装的你都不认识你自己了可笑不啊我们年轻人呀要放开的大胆的追求你的梦想不要让梦想来追你']\n",
            " > Processing time: 4.234215497970581\n",
            " > Real-time factor: 0.3989933834626125\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/130.wav\n",
            "Generating audio for: 131.txt\n",
            " > Text splitted to sentences.\n",
            "['都得气回家了不他妈玩了是不是啊都是什么人呐啊说啥不玩了最后说啥不玩了天天传闲话天天互相传闲我传的我头疼真的吗天天']\n",
            " > Processing time: 6.144154787063599\n",
            " > Real-time factor: 0.4750302000517263\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/131.wav\n",
            "Generating audio for: 132.txt\n",
            " > Text splitted to sentences.\n",
            "['李廉杰见我的时候还给我握手的你是干啥的啊人家宝宝刘还是许思多发烧友的你是干啥的啊王孝坤每回见到我还喊思多哥的你是干啥的']\n",
            " > Processing time: 5.0643415451049805\n",
            " > Real-time factor: 0.41071592372434534\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/132.wav\n",
            "Generating audio for: 133.txt\n",
            " > Text splitted to sentences.\n",
            "['为什么中国的滑板发展不开呢到现在发展不起来为什么就是因为互相看不起有什么可看不起的我就想知道啊唯一不能理解的就是中国玩滑板的很多人骂骂车铃']\n",
            " > Processing time: 7.096528768539429\n",
            " > Real-time factor: 0.5018165995763456\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/133.wav\n",
            "Generating audio for: 134.txt\n",
            " > Text splitted to sentences.\n",
            "['你们脑子是不是让萨拉巴特给踢了啊还是让麦克威给踹了你们天天都想点啥呀啊我们玩滑板的目的是玩滑板练滑板']\n",
            " > Processing time: 4.166089773178101\n",
            " > Real-time factor: 0.4063551892321516\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/134.wav\n",
            "Generating audio for: 135.txt\n",
            " > Text splitted to sentences.\n",
            "['在滑板中找到快感不是看不起人你看不起人最后看出快感了那你还玩什么滑板啊']\n",
            " > Processing time: 4.033690452575684\n",
            " > Real-time factor: 0.5116364155504707\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/135.wav\n",
            "Generating audio for: 136.txt\n",
            " > Text splitted to sentences.\n",
            "['你直接当杀手就得了吧会穿几件衣服就把自己当普肉了是不是啊你看人家真正的普肉']\n",
            " > Processing time: 4.039410829544067\n",
            " > Real-time factor: 0.41025963958032413\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/136.wav\n",
            "Generating audio for: 137.txt\n",
            " > Text splitted to sentences.\n",
            "['你看你那倒霉樣跟個賊似的眼淚那一卡吧這個范不正這個橋不正這只能證明一點你這個人心不正知道吧']\n",
            " > Processing time: 4.219713926315308\n",
            " > Real-time factor: 0.43060298072590025\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/137.wav\n",
            "Generating audio for: 138.txt\n",
            " > Text splitted to sentences.\n",
            "['有一千多人呢,没进去,在门口玩滑板呢,顾不得看了,就在门口正练呢,Double flip接前轮滑,我说我天哪,我说这太专业了这。']\n",
            " > Processing time: 5.024785280227661\n",
            " > Real-time factor: 0.44025571964611515\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/138.wav\n",
            "Generating audio for: 139.txt\n",
            " > Text splitted to sentences.\n",
            "['我跟你说,两万人什么概念啊?', '而且每一个人都是玩滑板的要不就小孩他父母陪他去看玩滑板去了,是吧?', '人多的都达到什么程度了?', '人满为患了,知道吧?', '人多的走不动道了']\n",
            " > Processing time: 7.484484672546387\n",
            " > Real-time factor: 0.4363732893071451\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/139.wav\n",
            "Generating audio for: 140.txt\n",
            " > Text splitted to sentences.\n",
            "['卡瑞肯尼迪、路斯卡、乱七八糟的大盘明星还有Wes老板子那人整烧烤的BBQ你免费吃,那场面啊,就跟九几年中国那人抓奖券似的场面都炸,黑牙牙一片的,都是人']\n",
            " > Processing time: 4.681535243988037\n",
            " > Real-time factor: 0.42532406606374934\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/140.wav\n",
            "Generating audio for: 141.txt\n",
            " > Text splitted to sentences.\n",
            "['进口的就一千多,两千块钱也别两千了,咱就算一千嘛学生暑假打工,打一个月工']\n",
            " > Processing time: 3.148341417312622\n",
            " > Real-time factor: 0.3993380594324857\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/141.wav\n",
            "Generating audio for: 142.txt\n",
            " > Text splitted to sentences.\n",
            "['几个月了 心说还没亲眼看过这世界呢于是就开始出国旅游了一口气干了70多个国家最后呢 现在活下来了']\n",
            " > Processing time: 4.973401784896851\n",
            " > Real-time factor: 0.4923474847217134\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/142.wav\n",
            "Generating audio for: 143.txt\n",
            " > Text splitted to sentences.\n",
            "['不但没有病了,而且现在比一般人活得还潇洒就是所以说啊,生活啊,就这他妈的有激情你天天这个呢,呵呵']\n",
            " > Processing time: 5.08418345451355\n",
            " > Real-time factor: 0.5085751849641784\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/143.wav\n",
            "Generating audio for: 144.txt\n",
            " > Text splitted to sentences.\n",
            "['成都没有任何一个地方可以替代重州都知道绿水青山,就是金山银山,而重州就是成都最珍贵的地方。']\n",
            " > Processing time: 4.631180286407471\n",
            " > Real-time factor: 0.533239646771267\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/144.wav\n",
            "Generating audio for: 145.txt\n",
            " > Text splitted to sentences.\n",
            "['茂林建湖,重州性美的森林景观堪称成都一绝在成都受到雾霾威胁的时候重州就是成都最大的绿肺不仅如此重州还是成都市引用水源派氧区']\n",
            " > Processing time: 5.490829229354858\n",
            " > Real-time factor: 0.4148374010034902\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/145.wav\n",
            "Generating audio for: 146.txt\n",
            " > Text splitted to sentences.\n",
            "['这里是华西雨平的中心地带,属于临山到裘莱山凉山生物多样性保护与水源含量重要区这里是成都市第二水源地,大美重州就像安装在成都西部的超大进化器它为成都不断输送干净的空气和水']\n",
            "[!] Warning: The text length exceeds the character limit of 82 for language 'zh', this might cause truncated audio.\n",
            " > Processing time: 8.813226222991943\n",
            " > Real-time factor: 0.49548106672217895\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/146.wav\n",
            "Generating audio for: 147.txt\n",
            " > Text splitted to sentences.\n",
            "['如果少了重州,成都人将无法呼吸。', '除了强大的城市进化作用重州对于成都来说。']\n",
            " > Processing time: 4.041966438293457\n",
            " > Real-time factor: 0.3928858088427966\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/147.wav\n",
            "Generating audio for: 148.txt\n",
            " > Text splitted to sentences.\n",
            "['分布最密集的史前城址群,年代距今約4500到4000年左右,其中子珠遺址已有4300年的歷史,是成都平原目前發現的三座具有內外城牆結構的古城之中。']\n",
            " > Processing time: 6.853796005249023\n",
            " > Real-time factor: 0.4726238488733455\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/148.wav\n",
            "Generating audio for: 149.txt\n",
            " > Text splitted to sentences.\n",
            "['最大最早的一座,这里堪称长江上游近五千年文明史的象征比三星堆遗址的产生年代要早一千多年重舟古城遗址群的发现,说明早在四千多年前']\n",
            " > Processing time: 6.641995906829834\n",
            " > Real-time factor: 0.4441506433645429\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/149.wav\n",
            "Generating audio for: 150.txt\n",
            " > Text splitted to sentences.\n",
            "['清末民初,湖广,江西本省的商人也到此清商并建造会馆清明的园偷人在江边住起了产镇从人通镇廊站的一道大门进去爬上一栋六层楼楼顶']\n",
            " > Processing time: 5.658111333847046\n",
            " > Real-time factor: 0.4499731480153477\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio/150.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n",
        "os.environ[\"LANG\"] = \"C.UTF-8\"\n",
        "os.environ[\"LANGUAGE\"] = \"C.UTF-8\"\n"
      ],
      "metadata": {
        "id": "L-f_4snrIHv6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pesq librosa numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjzdY1GvIDME",
        "outputId": "4264e5f0-5bde-4cb3-d0bf-5e503530e11f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pesq in /usr/local/lib/python3.11/dist-packages (0.0.4)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from pesq import pesq\n",
        "\n",
        "# This code generates PESQ scores by comparing the original Chinese dataset and the Chinese generated audio produced using TTS.\n",
        "\n",
        "REFERENCE_AUDIO_PATH = \"/content/drive/MyDrive/selected audio 3/chinese\"\n",
        "GENERATED_AUDIO_PATH = \"/content/drive/MyDrive/selected audio 3/generated audio/xtts_chinese_audio\"\n",
        "\n",
        "def compute_pesq(reference_file, generated_file, sample_rate=16000):\n",
        "    ref_audio, sr1 = librosa.load(reference_file, sr=sample_rate)\n",
        "    gen_audio, sr2 = librosa.load(generated_file, sr=sample_rate)\n",
        "\n",
        "    if sr1 != sample_rate or sr2 != sample_rate:\n",
        "        ref_audio = librosa.resample(ref_audio, orig_sr=sr1, target_sr=sample_rate)\n",
        "        gen_audio = librosa.resample(gen_audio, orig_sr=sr2, target_sr=sample_rate)\n",
        "\n",
        "    min_length = min(len(ref_audio), len(gen_audio))\n",
        "    ref_audio = ref_audio[:min_length]\n",
        "    gen_audio = gen_audio[:min_length]\n",
        "\n",
        "    return pesq(sample_rate, ref_audio, gen_audio, 'wb')\n",
        "\n",
        "pesq_scores = {}\n",
        "for file in sorted(os.listdir(REFERENCE_AUDIO_PATH)):\n",
        "    ref_file = os.path.join(REFERENCE_AUDIO_PATH, file)\n",
        "    gen_file = os.path.join(GENERATED_AUDIO_PATH, file)\n",
        "\n",
        "    if os.path.exists(gen_file):\n",
        "        score = compute_pesq(ref_file, gen_file)\n",
        "        pesq_scores[file] = score\n",
        "        print(f\"PESQ for {file}: {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"Missing generated audio for {file}\")\n",
        "\n",
        "pesq_score_file = \"/content/drive/MyDrive/selected audio 3/generated audio/pesq_chinese_xtts_model.txt\"\n",
        "with open(pesq_score_file, \"w\") as f:\n",
        "    for file, score in pesq_scores.items():\n",
        "        f.write(f\"{file}: {score:.4f}\\n\")\n",
        "    avg_pesq = np.mean(list(pesq_scores.values()))\n",
        "    f.write(f\"\\nAverage PESQ Score: {avg_pesq:.4f}\")\n",
        "\n",
        "print(f\"\\nAverage PESQ Score: {avg_pesq:.4f}\")\n",
        "print(f\"PESQ scores saved to {pesq_score_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KPJyp4LGBnQ",
        "outputId": "90723e26-b4d9-4706-e552-0ebc6dc58db2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PESQ for 101.wav: 1.7616\n",
            "PESQ for 102.wav: 1.8785\n",
            "PESQ for 103.wav: 1.0922\n",
            "PESQ for 104.wav: 1.0912\n",
            "PESQ for 105.wav: 1.2837\n",
            "PESQ for 106.wav: 2.4968\n",
            "PESQ for 107.wav: 1.4919\n",
            "PESQ for 108.wav: 1.2622\n",
            "PESQ for 109.wav: 1.6594\n",
            "PESQ for 110.wav: 1.1885\n",
            "PESQ for 111.wav: 2.2544\n",
            "PESQ for 112.wav: 1.1097\n",
            "PESQ for 113.wav: 1.0866\n",
            "PESQ for 114.wav: 1.1875\n",
            "PESQ for 115.wav: 1.0753\n",
            "PESQ for 116.wav: 1.0915\n",
            "PESQ for 117.wav: 1.0832\n",
            "PESQ for 118.wav: 1.0409\n",
            "PESQ for 119.wav: 1.6598\n",
            "PESQ for 120.wav: 1.1018\n",
            "PESQ for 121.wav: 1.0449\n",
            "PESQ for 122.wav: 1.2550\n",
            "PESQ for 123.wav: 1.0342\n",
            "PESQ for 124.wav: 1.0543\n",
            "PESQ for 125.wav: 1.2195\n",
            "PESQ for 126.wav: 1.1401\n",
            "PESQ for 127.wav: 1.0685\n",
            "PESQ for 128.wav: 1.1585\n",
            "PESQ for 129.wav: 1.0937\n",
            "PESQ for 130.wav: 1.1143\n",
            "PESQ for 131.wav: 1.0690\n",
            "PESQ for 132.wav: 1.0327\n",
            "PESQ for 133.wav: 1.0643\n",
            "PESQ for 134.wav: 1.1283\n",
            "PESQ for 135.wav: 1.0581\n",
            "PESQ for 136.wav: 1.0605\n",
            "PESQ for 137.wav: 1.1556\n",
            "PESQ for 138.wav: 1.1141\n",
            "PESQ for 139.wav: 1.0496\n",
            "PESQ for 140.wav: 1.2074\n",
            "PESQ for 141.wav: 1.1180\n",
            "PESQ for 142.wav: 1.2788\n",
            "PESQ for 143.wav: 1.1452\n",
            "PESQ for 144.wav: 1.0528\n",
            "PESQ for 145.wav: 1.1387\n",
            "PESQ for 146.wav: 1.0655\n",
            "PESQ for 147.wav: 1.0435\n",
            "PESQ for 148.wav: 1.0357\n",
            "PESQ for 149.wav: 1.0630\n",
            "PESQ for 150.wav: 1.1380\n",
            "\n",
            "Average PESQ Score: 1.2220\n",
            "PESQ scores saved to /content/drive/MyDrive/selected audio 3/generated audio/pesq_chinese_xtts_model.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y espeak-ng espeak ffmpeg\n",
        "!pip install TTS\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlr2uSK_JlSu",
        "outputId": "ca46e7f5-84ec-4c1f-f7da-ebbea4e1eda5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6555 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [66.7 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1319 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8704 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3664 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2941 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2661 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [57.8 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1235 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2639 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1531 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3799 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 29.1 MB in 3s (9280 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  espeak-data espeak-ng-data libespeak-ng1 libespeak1 libpcaudio0 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data espeak-ng espeak-ng-data libespeak-ng1 libespeak1 libpcaudio0 libportaudio2\n",
            "  libsonic0\n",
            "0 upgraded, 9 newly installed, 0 to remove and 109 not upgraded.\n",
            "Need to get 5897 kB of archives.\n",
            "After this operation, 15.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak-data amd64 1.48.15+dfsg-3 [1085 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libespeak1 amd64 1.48.15+dfsg-3 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak amd64 1.48.15+dfsg-3 [64.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8956 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3956 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Fetched 5897 kB in 1s (7483 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 124788 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../1-libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../2-espeak-data_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../3-libespeak1_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../4-espeak_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "Preparing to unpack .../5-libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../6-espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../7-libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../8-espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up espeak (1.48.15+dfsg-3) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.0.11)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.13.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from TTS) (2.5.1+cu121)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.13.0)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.10.2.post1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.6.1)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (7.5.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.67.1)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.11.11)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (24.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.1.0)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting umap-learn>=0.5.1 (from TTS)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas<2.0,>=1.4 (from TTS)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.10.0)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from TTS) (3.9.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.8.0)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.47.1)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.11/dist-packages (from spacy[ja]>=3->TTS) (3.7.5)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.26.4)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.60.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.16.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.18.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (4.4.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (4.55.6)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57.0->TTS) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0,>=1.4->TTS) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->TTS) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.0->TTS) (1.17.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.15.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.5.0)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiDict_core-20250129-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.17.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->TTS) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (2.17.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.5.2)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (3.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->TTS) (4.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (4.25.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl (937 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiDict_core-20250129-py3-none-any.whl (72.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75788 sha256=e433a4a2014b733296cde656bda636c6063a356a21940ee2fcf1af04437a2f87\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/a0/bc/4dacab52579ab464cffafbe7a8e3792dd36ad9ac288b264843\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=de50c6663138de8a6a6ad4768a544d7550016f5cdb09a06785f6d756c4715770\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/a4/88/480018a664e58ca7ce6708759193ee51b017b3b72aa3df8a85\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=832c70d6da314c0dd552604a3aad030d0aa08aa14a69fb2bdbf157185f6b33e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/b9/e3/4145416693824818c0b931988a692676ecd4bbf2ea41d1eedd\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e95d7d9fde8d29b2cc25e0aaa23c1454462cab65ae9987767afaeac8cd5f17e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=0d6a00f135ed9f000f423f32687c1b8675026dea98bb946d44e05dd2858cb9be\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/10/89/a5908dd7a9a032229684b7679396785e19f816667f788087fb\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498313 sha256=ca89e7b18fe8b91324c72cdf7b5a89a846938dffd374fe01daf6625c27713874\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/fa/df/5fdf5d3cc26ba859b8698a1f28581d1a6aa081edc6df9847ab\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326857 sha256=4b9b8accc211e138e4146877e370f7892d7054759432de39e1ba8aaef4e5c655\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/30/52/dc5cd222b4bbde285838fed1f96636e96f85cd75493e79a978\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173928 sha256=86236b74ce7ee03d38a996808d84484b64bf95665bfb98c661ff17af4623661a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/eb/59/30b5d15e56347e595f613036cbea0f807ad9621c75cd75d912\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=9e422a3725faa606bef3c4c0767c3607eda21a239e3ea36854a71b87c2da063b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/e7/a0/7c416a3eeaa94ca71bf7bcbc6289cced2263d8ba35e82444bb\n",
            "Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, python-crfsuite, pysbd, pypinyin, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, pandas, g2pkk, dateparser, pynndescent, gruut, umap-learn, trainer, encodec, TTS\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "xarray 2025.1.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "scikit-image 0.25.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 networkx-2.8.8 num2words-0.5.14 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 sudachidict-core-20250129 sudachipy-0.6.10 trainer-0.0.36 umap-learn-0.5.7 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code converts English transcriptions into speech using the XTTS model and saves the generated audio files.\n",
        "\n",
        "import torch\n",
        "from TTS.api import TTS\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
        "\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/selected audio 3/Transcriptions/english transcriptions\"\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio\"\n",
        "\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "for filename in sorted(os.listdir(INPUT_FOLDER)):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        transcript_path = os.path.join(INPUT_FOLDER, filename)\n",
        "\n",
        "        with open(transcript_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read().strip()\n",
        "\n",
        "        audio_filename = os.path.splitext(filename)[0] + \".wav\"\n",
        "        audio_path = os.path.join(OUTPUT_FOLDER, audio_filename)\n",
        "\n",
        "        print(f\"Generating audio for: {filename}\")\n",
        "        tts.tts_to_file(\n",
        "            text=text,\n",
        "            speaker_wav=\"/content/drive/MyDrive/selected audio 3/eng/101.wav\",\n",
        "            language=\"en\",\n",
        "            file_path=audio_path\n",
        "        )\n",
        "\n",
        "        print(f\"Saved: {audio_path}\")\n",
        "\n",
        "print(\"✅ Audio generation completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D7YE9P8HMip",
        "outputId": "15acef34-59f9-414e-f547-3f0774af4660"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > You must confirm the following:\n",
            " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
            " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n",
            " | | > y\n",
            " > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1.86G/1.87G [00:24<00:00, 102MiB/s]\n",
            "100%|██████████| 1.87G/1.87G [00:24<00:00, 75.5MiB/s]\n",
            "100%|██████████| 4.37k/4.37k [00:00<00:00, 27.1kiB/s]\n",
            "\n",
            "100%|██████████| 361k/361k [00:00<00:00, 1.74MiB/s]\n",
            "100%|██████████| 32.0/32.0 [00:00<00:00, 160iB/s]\n",
            " 81%|████████▏ | 6.32M/7.75M [00:00<00:00, 63.2MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Model's license - CPML\n",
            " > Check https://coqui.ai/cpml.txt for more info.\n",
            " > Using model: xtts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 7.75M/7.75M [00:14<00:00, 63.2MiB/s]/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.speakers = torch.load(speaker_file_path)\n",
            "/usr/local/lib/python3.11/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=map_location, **kwargs)\n",
            "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating audio for: 101.txt\n",
            " > Text splitted to sentences.\n",
            "['Good evening and welcome to Tucker Carlson tonight.', 'Looking back, January of 2017 seems like another age.', 'So much has happened in the years since then, but in other ways, not that much has changed at all.', 'Donald Trump had not even taken the oath of office yet.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Processing time: 13.175755262374878\n",
            " > Real-time factor: 0.5127594521902287\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/101.wav\n",
            "Generating audio for: 102.txt\n",
            " > Text splitted to sentences.\n",
            "[\"But by the first week of the new year of 2017, in case you don't remember, permanent Washington had already committed to destroying his presidency, and Trump seemed to know it.\"]\n",
            " > Processing time: 3.9656851291656494\n",
            " > Real-time factor: 0.35690000774710445\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/102.wav\n",
            "Generating audio for: 103.txt\n",
            " > Text splitted to sentences.\n",
            "['A little after 8 p.m. on the night of January 3rd, 2017, the president-elect wrote a tweet.', 'He took a veiled dig at U.S. intelligence agencies for their handling of the then-newly initiated Russia investigation.']\n",
            " > Processing time: 5.205913782119751\n",
            " > Real-time factor: 0.35555555213518597\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/103.wav\n",
            "Generating audio for: 104.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Now, Schuart hadn't seen the tweet before, of course.\", \"He couldn't have known it was coming.\", 'For one of the most calculating politicians in Washington, it was a rare, unscripted moment.']\n",
            " > Processing time: 6.6186747550964355\n",
            " > Real-time factor: 0.40714909373152147\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/104.wav\n",
            "Generating audio for: 105.txt\n",
            " > Text splitted to sentences.\n",
            "['And so for the first time in a long time, Chuck Schumer just went with the unvarnished truth.', 'Watch what he said.']\n",
            " > Processing time: 3.7932076454162598\n",
            " > Real-time factor: 0.34642241791512807\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/105.wav\n",
            "Generating audio for: 106.txt\n",
            " > Text splitted to sentences.\n",
            "[\"So Schumer went on to say he didn't know exactly what the spy agencies would do to Donald Trump as punishment for being dumb enough to criticize them in public.\", 'But he warned, and this is a verbatim quote,']\n",
            " > Processing time: 6.117290496826172\n",
            " > Real-time factor: 0.4208041811889073\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/106.wav\n",
            "Generating audio for: 107.txt\n",
            " > Text splitted to sentences.\n",
            "['from what I am told, they are very upset about how he has treated them and talked about them.', '\" End quote.', 'Very upset.']\n",
            " > Processing time: 5.038930416107178\n",
            " > Real-time factor: 0.34550355638080027\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/107.wav\n",
            "Generating audio for: 108.txt\n",
            " > Text splitted to sentences.\n",
            "['He seemed to feel free to say exactly what he really thought.', \"He didn't appear to believe that the intelligence agencies had veto power over his agenda.\"]\n",
            " > Processing time: 5.681449890136719\n",
            " > Real-time factor: 0.4047218096683896\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/108.wav\n",
            "Generating audio for: 109.txt\n",
            " > Text splitted to sentences.\n",
            "['In a thousand different ways, the new president refused to bow.', 'And for that crime, more than any other crime, he was punished, most recently by the manufactured Ukraine scandal.']\n",
            " > Processing time: 5.061499118804932\n",
            " > Real-time factor: 0.3567238658639177\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/109.wav\n",
            "Generating audio for: 110.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I remember William Barr, you know, when he was testifying in front of Congress, he said he didn't understand the predication of the counterintelligence investigation that was launched into Russia's interference in 2016 election.\"]\n",
            " > Processing time: 4.484512090682983\n",
            " > Real-time factor: 0.3592941238865465\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/110.wav\n",
            "Generating audio for: 111.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I don't understand the predication of this worldwide effort to try to uncover dirt, either real or imagined, that would discredit that investigation in 2016 into Russian interference.\"]\n",
            " > Processing time: 5.810008764266968\n",
            " > Real-time factor: 0.421217230167574\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/111.wav\n",
            "Generating audio for: 112.txt\n",
            " > Text splitted to sentences.\n",
            "[\"John Brennan is a naked partisan, and he's a liar.\", \"He's acted in ways that would have shocked and horrified previous CIA directors, and that's saying a lot.\"]\n",
            " > Processing time: 4.152284860610962\n",
            " > Real-time factor: 0.3530146559857793\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/112.wav\n",
            "Generating audio for: 113.txt\n",
            " > Text splitted to sentences.\n",
            "['What Phil Mudd is describing is not a conventional government agency.', \"It's nothing like what most of us imagine when we think about Washington is doing on our behalf.\"]\n",
            " > Processing time: 7.680477619171143\n",
            " > Real-time factor: 0.41787043896250414\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/113.wav\n",
            "Generating audio for: 114.txt\n",
            " > Text splitted to sentences.\n",
            "['The CIA of John Brennan and Phil Mudd does not exist for our benefit.', 'It exists solely for the benefit of the people who work there.']\n",
            " > Processing time: 5.22867226600647\n",
            " > Real-time factor: 0.35569967255356727\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/114.wav\n",
            "Generating audio for: 115.txt\n",
            " > Text splitted to sentences.\n",
            "['We pay for the whole thing, but they do what they want, and they punish anyone who criticizes them.', 'They brag about that.', \"Now, that's scary, of course.\", \"It's a perversion of democracy.\", \"It's exactly what the people who created the CIA feared most.\"]\n",
            " > Processing time: 8.546369791030884\n",
            " > Real-time factor: 0.39569850052962996\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/115.wav\n",
            "Generating audio for: 116.txt\n",
            " > Text splitted to sentences.\n",
            "[\"But it's also, if we're going to be honest, it's annoying.\", \"Because for all of the hype, the CIA in the end isn't even very good at its job.\", 'Now remember, this is an intelligence agency.', \"So it's fair to judge their performance.\"]\n",
            " > Processing time: 7.403832912445068\n",
            " > Real-time factor: 0.3829532815066567\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/116.wav\n",
            "Generating audio for: 117.txt\n",
            " > Text splitted to sentences.\n",
            "['against whether or not they predicted crucial events over the past 70 years.', \"And again and again, they didn't.\", 'The CIA, for example, was shocked by the Korean War.']\n",
            " > Processing time: 5.82778787612915\n",
            " > Real-time factor: 0.36081675577476463\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/117.wav\n",
            "Generating audio for: 118.txt\n",
            " > Text splitted to sentences.\n",
            "[\"They didn't even provide a warning before that happened because they had no idea it was going to happen.\", 'When the Iron Curtain finally fell in 1989, the CIA was completely blindsided by it.', 'They thought they had just predicted, in fact, that the Soviet Union was as strong as ever.']\n",
            " > Processing time: 8.827023267745972\n",
            " > Real-time factor: 0.4001164832701516\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/118.wav\n",
            "Generating audio for: 119.txt\n",
            " > Text splitted to sentences.\n",
            "[\"And things didn't get better after that.\", 'The CIA had no idea that Saddam Hussein planned to invade Kuwait the next year, in 1990.', \"They were totally surprised by India's atomic bomb test eight years later.\"]\n",
            " > Processing time: 6.665812253952026\n",
            " > Real-time factor: 0.3559072687023996\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/119.wav\n",
            "Generating audio for: 120.txt\n",
            " > Text splitted to sentences.\n",
            "['By 2003, they were totally confident that Iraq had weapons of mass destruction.', 'In fact, their biggest success in the past 50 years may have been creating the Taliban.']\n",
            " > Processing time: 6.10098934173584\n",
            " > Real-time factor: 0.43317495809272044\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/120.wav\n",
            "Generating audio for: 121.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Schiff also claimed to know nothing about what was in the whistleblower's complaint before it came out.\", 'But yesterday, the New York Times revealed that both those claims were lies.', 'Schiff apologized in a way...']\n",
            " > Processing time: 5.667901277542114\n",
            " > Real-time factor: 0.3552588551468016\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/121.wav\n",
            "Generating audio for: 122.txt\n",
            " > Text splitted to sentences.\n",
            "['saying that he quote should have been much more clear Right shift now admits his office spoke to the whistleblower who by the way we learned tonight is a registered Democrat']\n",
            " > Processing time: 4.841715574264526\n",
            " > Real-time factor: 0.46077544891812033\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/122.wav\n",
            "Generating audio for: 123.txt\n",
            " > Text splitted to sentences.\n",
            "['Let me tell you, you take on the intelligence community, they have six ways from Sunday at getting back at you.', \"So even for a practical, supposedly hard-nosed businessman, he's being really dumb to do this.\"]\n",
            " > Processing time: 6.787994146347046\n",
            " > Real-time factor: 0.3642513991486069\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/123.wav\n",
            "Generating audio for: 124.txt\n",
            " > Text splitted to sentences.\n",
            "['First off, have you heard from the whistleblower?', 'Do you want to hear from the whistleblower?', 'We have not spoken directly with the whistleblower.', 'We would like to.']\n",
            " > Processing time: 10.474979877471924\n",
            " > Real-time factor: 0.3936043091842348\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/124.wav\n",
            "Generating audio for: 125.txt\n",
            " > Text splitted to sentences.\n",
            "['I spent a lot of time in government.', 'There are State Department officials who will testify, Intel guys, DOD, Department of Defense people.', 'All of us are sort of a brotherhood and sisterhood.', 'Rudy Giuliani parachutes in from Mars.']\n",
            " > Processing time: 8.820777416229248\n",
            " > Real-time factor: 0.39710355013527154\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/125.wav\n",
            "Generating audio for: 126.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I'd like to begin by thanking all of the members of the Senate Judiciary Committee for considering my nomination.\", \"In particular, I'd like to thank Chairman Grassley, Ranking Member Leahy.\"]\n",
            " > Processing time: 4.829038858413696\n",
            " > Real-time factor: 0.35576922788150195\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/126.wav\n",
            "Generating audio for: 127.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I'm very proud to have with me here today my parents, Jay and Jeanne Clemencehrew of Devils Lake, North Dakota, and my mother-in-law.\"]\n",
            " > Processing time: 3.1649439334869385\n",
            " > Real-time factor: 0.35819071678875647\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/127.wav\n",
            "Generating audio for: 128.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Not with me here today, but I'd like to recognize our former United States attorneys Drew Wrigley and Tim Purden for their continued support of my nomination.\"]\n",
            " > Processing time: 4.568495273590088\n",
            " > Real-time factor: 0.4347736723234818\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/128.wav\n",
            "Generating audio for: 129.txt\n",
            " > Text splitted to sentences.\n",
            "['as well as former North Dakota Supreme Court Justice Mary Mulin Marring.']\n",
            " > Processing time: 2.8881006240844727\n",
            " > Real-time factor: 0.3528201110332784\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/129.wav\n",
            "Generating audio for: 130.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I'd like to thank her for her continued encouragement and counsel throughout this process as well as my entire legal career.\", \"And finally, I would like to thank my friends and colleagues at the United States Attorney's Office in North Dakota.\"]\n",
            " > Processing time: 7.66959810256958\n",
            " > Real-time factor: 0.41258743403480763\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/130.wav\n",
            "Generating audio for: 131.txt\n",
            " > Text splitted to sentences.\n",
            "['who are watching this hearing via webcast today.', 'Working with this dedicated group of attorneys and staff.']\n",
            " > Processing time: 4.879829168319702\n",
            " > Real-time factor: 0.35257494875697754\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/131.wav\n",
            "Generating audio for: 132.txt\n",
            " > Text splitted to sentences.\n",
            "['has been the greatest privilege and is the most rewarding experience of my professional career.', \"With that, I'm happy to answer your questions.\"]\n",
            " > Processing time: 4.60026216506958\n",
            " > Real-time factor: 0.3766030828226515\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/132.wav\n",
            "Generating audio for: 133.txt\n",
            " > Text splitted to sentences.\n",
            "['I gotta go to the bathroom so bad, just got home.', \"So yeah, that's that.\", \"I'm outtie.\", \"Peace y'all.\"]\n",
            " > Processing time: 8.779288530349731\n",
            " > Real-time factor: 0.3757906884671457\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/133.wav\n",
            "Generating audio for: 134.txt\n",
            " > Text splitted to sentences.\n",
            "['Look, we love you both.', 'We both want you to stay, but we discussed what you guys bring to the villa, so...']\n",
            " > Processing time: 4.784590721130371\n",
            " > Real-time factor: 0.42480119105513414\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/134.wav\n",
            "Generating audio for: 135.txt\n",
            " > Text splitted to sentences.\n",
            "['Cynthia and I were speaking for a long while about how both you guys have excellent qualities and we both like you a lot.']\n",
            " > Processing time: 3.3228700160980225\n",
            " > Real-time factor: 0.36549846284101584\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/135.wav\n",
            "Generating audio for: 136.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Jared, you've been saved by your fellow Islanders.\", 'How are you feeling?', \"Yeah, it's tough.\", \"So it's just the narcotics plant is actually just empty.\", 'Jared, you are safe, and you can rejoin the group.']\n",
            " > Processing time: 7.517989158630371\n",
            " > Real-time factor: 0.37445259348865084\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/136.wav\n",
            "Generating audio for: 137.txt\n",
            " > Text splitted to sentences.\n",
            "['tax, and is championed by environmentalists and even some conservatives.', 'But what exactly is a carbon tax, and could it actually work?', 'Well, a carbon tax establishes a']\n",
            " > Processing time: 5.887160778045654\n",
            " > Real-time factor: 0.3775181912076761\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/137.wav\n",
            "Generating audio for: 138.txt\n",
            " > Text splitted to sentences.\n",
            "['2016 has been the worst year for carbon emissions in 66 million years.', 'And with Donald Trump as the new president-elect, that may not get better anytime soon.', 'Such extreme pollution has demanded a solution from world leaders.', 'The second proposed idea is called a carbon tax.']\n",
            " > Processing time: 10.891073942184448\n",
            " > Real-time factor: 0.38520843025800594\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/138.wav\n",
            "Generating audio for: 139.txt\n",
            " > Text splitted to sentences.\n",
            "['Thanks for watching!']\n",
            " > Processing time: 4.708259582519531\n",
            " > Real-time factor: 0.352927399355982\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/139.wav\n",
            "Generating audio for: 140.txt\n",
            " > Text splitted to sentences.\n",
            "['to find solutions without adding more regulations.', \"It's this last point that's particularly appealing to conservatives.\", 'But realistically, if companies have to pay an additional fee, chances are that energy costs will rise.']\n",
            " > Processing time: 8.7879638671875\n",
            " > Real-time factor: 0.39093943330862113\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/140.wav\n",
            "Generating audio for: 141.txt\n",
            " > Text splitted to sentences.\n",
            "['set the increase in energy costs to the consumer is to make the tax revenue neutral.', 'This means that while energy costs would rise, people would see the money returned to them instead of the government, either via a reimbursement check or by a reduction in income taxes.', 'Carbon taxes already exist in Denmark, Germany, and the United States.']\n",
            " > Processing time: 9.725470781326294\n",
            " > Real-time factor: 0.39213886949633686\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/141.wav\n",
            "Generating audio for: 142.txt\n",
            " > Text splitted to sentences.\n",
            "['Finland, Ireland, the Netherlands, Norway, Slovenia, Switzerland, and Chile.', 'Sweden was the first country to institute a carbon tax, and they did so back in 1991.', \"Currently, it's a tax of $100 million.\"]\n",
            " > Processing time: 7.28973126411438\n",
            " > Real-time factor: 0.35570445414288954\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/142.wav\n",
            "Generating audio for: 143.txt\n",
            " > Text splitted to sentences.\n",
            "['did appear to slightly reduce carbon emissions, critics say that the tax']\n",
            " > Processing time: 2.7405128479003906\n",
            " > Real-time factor: 0.46369174567375393\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/143.wav\n",
            "Generating audio for: 144.txt\n",
            " > Text splitted to sentences.\n",
            "['between $10 and $30 a ton was too low to change industry behavior.', 'In fact, oil company Exxon Mobil supports a carbon tax between $10 and $30 a ton.']\n",
            " > Processing time: 5.064382791519165\n",
            " > Real-time factor: 0.35809915518534374\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/144.wav\n",
            "Generating audio for: 145.txt\n",
            " > Text splitted to sentences.\n",
            "['$40 and $80 per ton, believing that stability and a regulatory environment will help them in the long term.', 'So why are carbon taxes controversial?', 'Opponents of the tax.']\n",
            " > Processing time: 7.493955135345459\n",
            " > Real-time factor: 0.39692558979584003\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/145.wav\n",
            "Generating audio for: 146.txt\n",
            " > Text splitted to sentences.\n",
            "[\"But there's a heated debate about what to do with the tax revenues, and whether they should go directly back to the consumer, or be used to help progress to a greener economy.\", 'As part of the Paris Climate Agreement, the United States has committed to reduce its greenhouse gas emissions by 26 to 28 percent.']\n",
            " > Processing time: 7.11137580871582\n",
            " > Real-time factor: 0.36091790477964536\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/146.wav\n",
            "Generating audio for: 147.txt\n",
            " > Text splitted to sentences.\n",
            "['by 2025.', 'But climate change legislation has seen little progress in Congress, and with the election of Donald Trump, who has criticized the Paris Agreement, many are unsure the deal will remain in place.', 'So where does that leave us?']\n",
            " > Processing time: 7.608595609664917\n",
            " > Real-time factor: 0.406245237478961\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/147.wav\n",
            "Generating audio for: 148.txt\n",
            " > Text splitted to sentences.\n",
            "['flow unpriced into the atmosphere any more than you should be allowed to tosh your garbage in the street.', 'We at Seeker are committed to bringing you stories that will inform and inspire you.', 'In this next episode, meet 25-year-old']\n",
            " > Processing time: 6.529371976852417\n",
            " > Real-time factor: 0.353442427259505\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/148.wav\n",
            "Generating audio for: 149.txt\n",
            " > Text splitted to sentences.\n",
            "['4-year-old Louis Bird, an inexperienced rower who embarked on the journey of his life across the Pacific Ocean, all to connect with the memory of his father.']\n",
            " > Processing time: 4.585877895355225\n",
            " > Real-time factor: 0.45398412287453627\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/149.wav\n",
            "Generating audio for: 150.txt\n",
            " > Text splitted to sentences.\n",
            "['Thank you for watching Seeker Daily, please make sure to like and subscribe to see new videos every day.']\n",
            " > Processing time: 2.7962124347686768\n",
            " > Real-time factor: 0.35054400633726757\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio/150.wav\n",
            "✅ Audio generation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from pesq import pesq\n",
        "\n",
        "# This code generates PESQ scores by comparing the original English dataset and the English generated audio produced using TTS.\n",
        "\n",
        "REFERENCE_AUDIO_PATH = \"/content/drive/MyDrive/selected audio 3/eng\"\n",
        "GENERATED_AUDIO_PATH = \"/content/drive/MyDrive/selected audio 3/generated audio/xtts_eng_audio\"\n",
        "\n",
        "def compute_pesq(reference_file, generated_file, sample_rate=16000):\n",
        "    ref_audio, sr1 = librosa.load(reference_file, sr=sample_rate)\n",
        "    gen_audio, sr2 = librosa.load(generated_file, sr=sample_rate)\n",
        "\n",
        "    if sr1 != sample_rate or sr2 != sample_rate:\n",
        "        ref_audio = librosa.resample(ref_audio, orig_sr=sr1, target_sr=sample_rate)\n",
        "        gen_audio = librosa.resample(gen_audio, orig_sr=sr2, target_sr=sample_rate)\n",
        "\n",
        "    min_length = min(len(ref_audio), len(gen_audio))\n",
        "    ref_audio = ref_audio[:min_length]\n",
        "    gen_audio = gen_audio[:min_length]\n",
        "\n",
        "    return pesq(sample_rate, ref_audio, gen_audio, 'wb')\n",
        "\n",
        "pesq_scores = {}\n",
        "for file in sorted(os.listdir(REFERENCE_AUDIO_PATH)):\n",
        "    ref_file = os.path.join(REFERENCE_AUDIO_PATH, file)\n",
        "    gen_file = os.path.join(GENERATED_AUDIO_PATH, file)\n",
        "\n",
        "    if os.path.exists(gen_file):\n",
        "        score = compute_pesq(ref_file, gen_file)\n",
        "        pesq_scores[file] = score\n",
        "        print(f\"PESQ for {file}: {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"Missing generated audio for {file}\")\n",
        "\n",
        "pesq_score_file = \"/content/drive/MyDrive/selected audio 3/generated audio/pesq_english_xtts_model.txt\"\n",
        "with open(pesq_score_file, \"w\") as f:\n",
        "    for file, score in pesq_scores.items():\n",
        "        f.write(f\"{file}: {score:.4f}\\n\")\n",
        "    avg_pesq = np.mean(list(pesq_scores.values()))\n",
        "    f.write(f\"\\nAverage PESQ Score: {avg_pesq:.4f}\")\n",
        "\n",
        "print(f\"\\nAverage PESQ Score: {avg_pesq:.4f}\")\n",
        "print(f\"PESQ scores saved to {pesq_score_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sLePTqlKM_w",
        "outputId": "05072a1a-90d8-4679-f039-d4cc5b0bbee5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PESQ for 101.wav: 1.0395\n",
            "PESQ for 102.wav: 1.1059\n",
            "PESQ for 103.wav: 1.2008\n",
            "PESQ for 104.wav: 1.0855\n",
            "PESQ for 105.wav: 1.0393\n",
            "PESQ for 106.wav: 1.1555\n",
            "PESQ for 107.wav: 1.8158\n",
            "PESQ for 108.wav: 1.0351\n",
            "PESQ for 109.wav: 1.0491\n",
            "PESQ for 110.wav: 1.0415\n",
            "PESQ for 111.wav: 1.1359\n",
            "PESQ for 112.wav: 1.0907\n",
            "PESQ for 113.wav: 1.0414\n",
            "PESQ for 114.wav: 1.0567\n",
            "PESQ for 115.wav: 1.0434\n",
            "PESQ for 116.wav: 1.3498\n",
            "PESQ for 117.wav: 1.0544\n",
            "PESQ for 118.wav: 1.0420\n",
            "PESQ for 119.wav: 1.0323\n",
            "PESQ for 120.wav: 1.0314\n",
            "PESQ for 121.wav: 1.0529\n",
            "PESQ for 122.wav: 1.1286\n",
            "PESQ for 123.wav: 1.0414\n",
            "PESQ for 124.wav: 1.1525\n",
            "PESQ for 125.wav: 1.0262\n",
            "PESQ for 126.wav: 1.1638\n",
            "PESQ for 127.wav: 1.0978\n",
            "PESQ for 128.wav: 1.1303\n",
            "PESQ for 129.wav: 1.0554\n",
            "PESQ for 130.wav: 1.8906\n",
            "PESQ for 131.wav: 1.0408\n",
            "PESQ for 132.wav: 1.1412\n",
            "PESQ for 133.wav: 1.1050\n",
            "PESQ for 134.wav: 1.0234\n",
            "PESQ for 135.wav: 1.0389\n",
            "PESQ for 136.wav: 1.1050\n",
            "PESQ for 137.wav: 1.2366\n",
            "PESQ for 138.wav: 1.3867\n",
            "PESQ for 139.wav: 1.0573\n",
            "PESQ for 140.wav: 1.5395\n",
            "PESQ for 141.wav: 1.0346\n",
            "PESQ for 142.wav: 1.0593\n",
            "PESQ for 143.wav: 1.0742\n",
            "PESQ for 144.wav: 1.0321\n",
            "PESQ for 145.wav: 1.0475\n",
            "PESQ for 146.wav: 1.0320\n",
            "PESQ for 147.wav: 1.0300\n",
            "PESQ for 148.wav: 1.0572\n",
            "PESQ for 149.wav: 1.0667\n",
            "PESQ for 150.wav: 1.5110\n",
            "\n",
            "Average PESQ Score: 1.1361\n",
            "PESQ scores saved to /content/drive/MyDrive/selected audio 3/generated audio/pesq_english_xtts_model.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code converts English transcriptions into speech using the Tacotron2 model and saves the generated audio files.\n",
        "\n",
        "import torch\n",
        "from TTS.api import TTS\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=device)\n",
        "\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/selected audio 3/Transcriptions/english transcriptions\"\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron\"\n",
        "\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "for filename in sorted(os.listdir(INPUT_FOLDER)):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        transcript_path = os.path.join(INPUT_FOLDER, filename)\n",
        "\n",
        "        with open(transcript_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read().strip()\n",
        "\n",
        "        audio_filename = os.path.splitext(filename)[0] + \".wav\"\n",
        "        audio_path = os.path.join(OUTPUT_FOLDER, audio_filename)\n",
        "\n",
        "        print(f\"Generating audio for: {filename}\")\n",
        "        tts.tts_to_file(text=text, file_path=audio_path)\n",
        "\n",
        "        print(f\"Saved: {audio_path}\")\n",
        "\n",
        "print(\"✅ Audio generation completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeDIGm0ELFZa",
        "outputId": "4a86e590-cc48-4cbe-c53f-f3b208c3fcf4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
            "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Downloading model to /root/.local/share/tts/tts_models--en--ljspeech--tacotron2-DDC\n",
            " > Model's license - apache 2.0\n",
            " > Check https://choosealicense.com/licenses/apache-2.0/ for more info.\n",
            " > Downloading model to /root/.local/share/tts/vocoder_models--en--ljspeech--hifigan_v2\n",
            " > Model's license - apache 2.0\n",
            " > Check https://choosealicense.com/licenses/apache-2.0/ for more info.\n",
            " > Using model: Tacotron2\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:2.718281828459045\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Model's reduction rate `r` is set to: 1\n",
            " > Vocoder Model: hifigan\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:2.718281828459045\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Generator Model: hifigan_generator\n",
            " > Discriminator Model: hifigan_discriminator\n",
            "Removing weight norm...\n",
            "Generating audio for: 101.txt\n",
            " > Text splitted to sentences.\n",
            "['Good evening and welcome to Tucker Carlson tonight.', 'Looking back, January of 2017 seems like another age.', 'So much has happened in the years since then, but in other ways, not that much has changed at all.', 'Donald Trump had not even taken the oath of office yet.']\n",
            " > Processing time: 2.0116217136383057\n",
            " > Real-time factor: 0.10208669075371152\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/101.wav\n",
            "Generating audio for: 102.txt\n",
            " > Text splitted to sentences.\n",
            "[\"But by the first week of the new year of 2017, in case you don't remember, permanent Washington had already committed to destroying his presidency, and Trump seemed to know it.\"]\n",
            " > Processing time: 1.1038153171539307\n",
            " > Real-time factor: 0.09229995048557495\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/102.wav\n",
            "Generating audio for: 103.txt\n",
            " > Text splitted to sentences.\n",
            "['A little after 8 p.m. on the night of January 3rd, 2017, the president-elect wrote a tweet.', 'He took a veiled dig at U.S. intelligence agencies for their handling of the then-newly initiated Russia investigation.']\n",
            " > Processing time: 1.636911392211914\n",
            " > Real-time factor: 0.09305620461975267\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/103.wav\n",
            "Generating audio for: 104.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Now, Schuart hadn't seen the tweet before, of course.\", \"He couldn't have known it was coming.\", 'For one of the most calculating politicians in Washington, it was a rare, unscripted moment.']\n",
            " > Processing time: 1.4240376949310303\n",
            " > Real-time factor: 0.09581125559375218\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/104.wav\n",
            "Generating audio for: 105.txt\n",
            " > Text splitted to sentences.\n",
            "['And so for the first time in a long time, Chuck Schumer just went with the unvarnished truth.', 'Watch what he said.']\n",
            " > Processing time: 0.8367400169372559\n",
            " > Real-time factor: 0.09672306121805534\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/105.wav\n",
            "Generating audio for: 106.txt\n",
            " > Text splitted to sentences.\n",
            "[\"So Schumer went on to say he didn't know exactly what the spy agencies would do to Donald Trump as punishment for being dumb enough to criticize them in public.\", 'But he warned, and this is a verbatim quote,']\n",
            " > Processing time: 1.3603589534759521\n",
            " > Real-time factor: 0.09455990531418575\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/106.wav\n",
            "Generating audio for: 107.txt\n",
            " > Text splitted to sentences.\n",
            "['from what I am told, they are very upset about how he has treated them and talked about them.', '\" End quote.', 'Very upset.']\n",
            " > Processing time: 0.8830647468566895\n",
            " > Real-time factor: 0.09529195867683621\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/107.wav\n",
            "Generating audio for: 108.txt\n",
            " > Text splitted to sentences.\n",
            "['He seemed to feel free to say exactly what he really thought.', \"He didn't appear to believe that the intelligence agencies had veto power over his agenda.\"]\n",
            " > Processing time: 1.3674778938293457\n",
            " > Real-time factor: 0.12870888351547377\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/108.wav\n",
            "Generating audio for: 109.txt\n",
            " > Text splitted to sentences.\n",
            "['In a thousand different ways, the new president refused to bow.', 'And for that crime, more than any other crime, he was punished, most recently by the manufactured Ukraine scandal.']\n",
            " > Processing time: 1.8875339031219482\n",
            " > Real-time factor: 0.1274002184464656\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/109.wav\n",
            "Generating audio for: 110.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I remember William Barr, you know, when he was testifying in front of Congress, he said he didn't understand the predication of the counterintelligence investigation that was launched into Russia's interference in 2016 election.\"]\n",
            " > Processing time: 1.4723796844482422\n",
            " > Real-time factor: 0.09338318618575331\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/110.wav\n",
            "Generating audio for: 111.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I don't understand the predication of this worldwide effort to try to uncover dirt, either real or imagined, that would discredit that investigation in 2016 into Russian interference.\"]\n",
            " > Processing time: 1.179987907409668\n",
            " > Real-time factor: 0.09596759131891111\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/111.wav\n",
            "Generating audio for: 112.txt\n",
            " > Text splitted to sentences.\n",
            "[\"John Brennan is a naked partisan, and he's a liar.\", \"He's acted in ways that would have shocked and horrified previous CIA directors, and that's saying a lot.\"]\n",
            " > Processing time: 1.1178724765777588\n",
            " > Real-time factor: 0.09346972495957552\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/112.wav\n",
            "Generating audio for: 113.txt\n",
            " > Text splitted to sentences.\n",
            "['What Phil Mudd is describing is not a conventional government agency.', \"It's nothing like what most of us imagine when we think about Washington is doing on our behalf.\"]\n",
            " > Processing time: 1.0844826698303223\n",
            " > Real-time factor: 0.09247325079568822\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/113.wav\n",
            "Generating audio for: 114.txt\n",
            " > Text splitted to sentences.\n",
            "['The CIA of John Brennan and Phil Mudd does not exist for our benefit.', 'It exists solely for the benefit of the people who work there.']\n",
            " > Processing time: 0.8946478366851807\n",
            " > Real-time factor: 0.08979872905548175\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/114.wav\n",
            "Generating audio for: 115.txt\n",
            " > Text splitted to sentences.\n",
            "['We pay for the whole thing, but they do what they want, and they punish anyone who criticizes them.', 'They brag about that.', \"Now, that's scary, of course.\", \"It's a perversion of democracy.\", \"It's exactly what the people who created the CIA feared most.\"]\n",
            " > Processing time: 1.8717448711395264\n",
            " > Real-time factor: 0.091119375482679\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/115.wav\n",
            "Generating audio for: 116.txt\n",
            " > Text splitted to sentences.\n",
            "[\"But it's also, if we're going to be honest, it's annoying.\", \"Because for all of the hype, the CIA in the end isn't even very good at its job.\", 'Now remember, this is an intelligence agency.', \"So it's fair to judge their performance.\"]\n",
            " > Processing time: 1.6758348941802979\n",
            " > Real-time factor: 0.09071131042978095\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/116.wav\n",
            "Generating audio for: 117.txt\n",
            " > Text splitted to sentences.\n",
            "['against whether or not they predicted crucial events over the past 70 years.', \"And again and again, they didn't.\", 'The CIA, for example, was shocked by the Korean War.']\n",
            " > Processing time: 1.7804419994354248\n",
            " > Real-time factor: 0.12788198418053603\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/117.wav\n",
            "Generating audio for: 118.txt\n",
            " > Text splitted to sentences.\n",
            "[\"They didn't even provide a warning before that happened because they had no idea it was going to happen.\", 'When the Iron Curtain finally fell in 1989, the CIA was completely blindsided by it.', 'They thought they had just predicted, in fact, that the Soviet Union was as strong as ever.']\n",
            " > Processing time: 2.337780714035034\n",
            " > Real-time factor: 0.11222886566097229\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/118.wav\n",
            "Generating audio for: 119.txt\n",
            " > Text splitted to sentences.\n",
            "[\"And things didn't get better after that.\", 'The CIA had no idea that Saddam Hussein planned to invade Kuwait the next year, in 1990.', \"They were totally surprised by India's atomic bomb test eight years later.\"]\n",
            " > Processing time: 1.4952924251556396\n",
            " > Real-time factor: 0.09386016276099365\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/119.wav\n",
            "Generating audio for: 120.txt\n",
            " > Text splitted to sentences.\n",
            "['By 2003, they were totally confident that Iraq had weapons of mass destruction.', 'In fact, their biggest success in the past 50 years may have been creating the Taliban.']\n",
            " > Processing time: 1.2480614185333252\n",
            " > Real-time factor: 0.09322409985995873\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/120.wav\n",
            "Generating audio for: 121.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Schiff also claimed to know nothing about what was in the whistleblower's complaint before it came out.\", 'But yesterday, the New York Times revealed that both those claims were lies.', 'Schiff apologized in a way...']\n",
            " > Processing time: 3.0428314208984375\n",
            " > Real-time factor: 0.09376388104388478\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/121.wav\n",
            "Generating audio for: 122.txt\n",
            " > Text splitted to sentences.\n",
            "['saying that he quote should have been much more clear Right shift now admits his office spoke to the whistleblower who by the way we learned tonight is a registered Democrat']\n",
            " > Processing time: 0.9788186550140381\n",
            " > Real-time factor: 0.09579820033671055\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/122.wav\n",
            "Generating audio for: 123.txt\n",
            " > Text splitted to sentences.\n",
            "['Let me tell you, you take on the intelligence community, they have six ways from Sunday at getting back at you.', \"So even for a practical, supposedly hard-nosed businessman, he's being really dumb to do this.\"]\n",
            " > Processing time: 1.4582855701446533\n",
            " > Real-time factor: 0.09443190495985342\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/123.wav\n",
            "Generating audio for: 124.txt\n",
            " > Text splitted to sentences.\n",
            "['First off, have you heard from the whistleblower?', 'Do you want to hear from the whistleblower?', 'We have not spoken directly with the whistleblower.', 'We would like to.']\n",
            " > Processing time: 1.4206361770629883\n",
            " > Real-time factor: 0.12112189009619714\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/124.wav\n",
            "Generating audio for: 125.txt\n",
            " > Text splitted to sentences.\n",
            "['I spent a lot of time in government.', 'There are State Department officials who will testify, Intel guys, DOD, Department of Defense people.', 'All of us are sort of a brotherhood and sisterhood.', 'Rudy Giuliani parachutes in from Mars.']\n",
            " > Processing time: 2.2957208156585693\n",
            " > Real-time factor: 0.12888179277657918\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/125.wav\n",
            "Generating audio for: 126.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I'd like to begin by thanking all of the members of the Senate Judiciary Committee for considering my nomination.\", \"In particular, I'd like to thank Chairman Grassley, Ranking Member Leahy.\"]\n",
            " > Processing time: 1.2447500228881836\n",
            " > Real-time factor: 0.09403689975292062\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/126.wav\n",
            "Generating audio for: 127.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I'm very proud to have with me here today my parents, Jay and Jeanne Clemencehrew of Devils Lake, North Dakota, and my mother-in-law.\"]\n",
            " > Processing time: 1.0113370418548584\n",
            " > Real-time factor: 0.09509100503556163\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/127.wav\n",
            "Generating audio for: 128.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Not with me here today, but I'd like to recognize our former United States attorneys Drew Wrigley and Tim Purden for their continued support of my nomination.\"]\n",
            " > Processing time: 1.0465607643127441\n",
            " > Real-time factor: 0.09389307683864986\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/128.wav\n",
            "Generating audio for: 129.txt\n",
            " > Text splitted to sentences.\n",
            "['as well as former North Dakota Supreme Court Justice Mary Mulin Marring.']\n",
            " > Processing time: 0.5061671733856201\n",
            " > Real-time factor: 0.09274852224731521\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/129.wav\n",
            "Generating audio for: 130.txt\n",
            " > Text splitted to sentences.\n",
            "[\"I'd like to thank her for her continued encouragement and counsel throughout this process as well as my entire legal career.\", \"And finally, I would like to thank my friends and colleagues at the United States Attorney's Office in North Dakota.\"]\n",
            " > Processing time: 1.494192361831665\n",
            " > Real-time factor: 0.09539441529923393\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/130.wav\n",
            "Generating audio for: 131.txt\n",
            " > Text splitted to sentences.\n",
            "['who are watching this hearing via webcast today.', 'Working with this dedicated group of attorneys and staff.']\n",
            " > Processing time: 0.7925434112548828\n",
            " > Real-time factor: 0.09820391014526482\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/131.wav\n",
            "Generating audio for: 132.txt\n",
            " > Text splitted to sentences.\n",
            "['has been the greatest privilege and is the most rewarding experience of my professional career.', \"With that, I'm happy to answer your questions.\"]\n",
            " > Processing time: 0.9417545795440674\n",
            " > Real-time factor: 0.09365388439415268\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/132.wav\n",
            "Generating audio for: 133.txt\n",
            " > Text splitted to sentences.\n",
            "['I gotta go to the bathroom so bad, just got home.', \"So yeah, that's that.\", \"I'm outtie.\", \"Peace y'all.\"]\n",
            " > Processing time: 0.9634292125701904\n",
            " > Real-time factor: 0.09448661283612964\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/133.wav\n",
            "Generating audio for: 134.txt\n",
            " > Text splitted to sentences.\n",
            "['Look, we love you both.', 'We both want you to stay, but we discussed what you guys bring to the villa, so...']\n",
            "   > Decoder stopped with `max_decoder_steps` 10000\n",
            " > Processing time: 12.378548860549927\n",
            " > Real-time factor: 0.10406893308064426\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/134.wav\n",
            "Generating audio for: 135.txt\n",
            " > Text splitted to sentences.\n",
            "['Cynthia and I were speaking for a long while about how both you guys have excellent qualities and we both like you a lot.']\n",
            " > Processing time: 0.7903892993927002\n",
            " > Real-time factor: 0.0942833249567701\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/135.wav\n",
            "Generating audio for: 136.txt\n",
            " > Text splitted to sentences.\n",
            "[\"Jared, you've been saved by your fellow Islanders.\", 'How are you feeling?', \"Yeah, it's tough.\", \"So it's just the narcotics plant is actually just empty.\", 'Jared, you are safe, and you can rejoin the group.']\n",
            " > Processing time: 2.0248236656188965\n",
            " > Real-time factor: 0.11374312616398491\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/136.wav\n",
            "Generating audio for: 137.txt\n",
            " > Text splitted to sentences.\n",
            "['tax, and is championed by environmentalists and even some conservatives.', 'But what exactly is a carbon tax, and could it actually work?', 'Well, a carbon tax establishes a']\n",
            " > Processing time: 1.8560771942138672\n",
            " > Real-time factor: 0.12726535565331537\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/137.wav\n",
            "Generating audio for: 138.txt\n",
            " > Text splitted to sentences.\n",
            "['2016 has been the worst year for carbon emissions in 66 million years.', 'And with Donald Trump as the new president-elect, that may not get better anytime soon.', 'Such extreme pollution has demanded a solution from world leaders.', 'The second proposed idea is called a carbon tax.']\n",
            "   > Decoder stopped with `max_decoder_steps` 10000\n",
            " > Processing time: 13.484148979187012\n",
            " > Real-time factor: 0.10202953249259931\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/138.wav\n",
            "Generating audio for: 139.txt\n",
            " > Text splitted to sentences.\n",
            "['Thanks for watching!']\n",
            " > Processing time: 0.17429661750793457\n",
            " > Real-time factor: 0.09808188076893522\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/139.wav\n",
            "Generating audio for: 140.txt\n",
            " > Text splitted to sentences.\n",
            "['to find solutions without adding more regulations.', \"It's this last point that's particularly appealing to conservatives.\", 'But realistically, if companies have to pay an additional fee, chances are that energy costs will rise.']\n",
            " > Processing time: 1.5504724979400635\n",
            " > Real-time factor: 0.09324655951226925\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/140.wav\n",
            "Generating audio for: 141.txt\n",
            " > Text splitted to sentences.\n",
            "['set the increase in energy costs to the consumer is to make the tax revenue neutral.', 'This means that while energy costs would rise, people would see the money returned to them instead of the government, either via a reimbursement check or by a reduction in income taxes.', 'Carbon taxes already exist in Denmark, Germany, and the United States.']\n",
            " > Processing time: 2.2526440620422363\n",
            " > Real-time factor: 0.09260582579130618\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/141.wav\n",
            "Generating audio for: 142.txt\n",
            " > Text splitted to sentences.\n",
            "['Finland, Ireland, the Netherlands, Norway, Slovenia, Switzerland, and Chile.', 'Sweden was the first country to institute a carbon tax, and they did so back in 1991.', \"Currently, it's a tax of $100 million.\"]\n",
            " > Processing time: 1.8263099193572998\n",
            " > Real-time factor: 0.09279516858807207\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/142.wav\n",
            "Generating audio for: 143.txt\n",
            " > Text splitted to sentences.\n",
            "['did appear to slightly reduce carbon emissions, critics say that the tax']\n",
            " > Processing time: 0.5291872024536133\n",
            " > Real-time factor: 0.09377473490824044\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/143.wav\n",
            "Generating audio for: 144.txt\n",
            " > Text splitted to sentences.\n",
            "['between $10 and $30 a ton was too low to change industry behavior.', 'In fact, oil company Exxon Mobil supports a carbon tax between $10 and $30 a ton.']\n",
            " > Processing time: 1.2735474109649658\n",
            " > Real-time factor: 0.09295135715157787\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/144.wav\n",
            "Generating audio for: 145.txt\n",
            " > Text splitted to sentences.\n",
            "['$40 and $80 per ton, believing that stability and a regulatory environment will help them in the long term.', 'So why are carbon taxes controversial?', 'Opponents of the tax.']\n",
            " > Processing time: 1.346275806427002\n",
            " > Real-time factor: 0.09718382199634445\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/145.wav\n",
            "Generating audio for: 146.txt\n",
            " > Text splitted to sentences.\n",
            "[\"But there's a heated debate about what to do with the tax revenues, and whether they should go directly back to the consumer, or be used to help progress to a greener economy.\", 'As part of the Paris Climate Agreement, the United States has committed to reduce its greenhouse gas emissions by 26 to 28 percent.']\n",
            " > Processing time: 2.9684109687805176\n",
            " > Real-time factor: 0.1387196122872381\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/146.wav\n",
            "Generating audio for: 147.txt\n",
            " > Text splitted to sentences.\n",
            "['by 2025.', 'But climate change legislation has seen little progress in Congress, and with the election of Donald Trump, who has criticized the Paris Agreement, many are unsure the deal will remain in place.', 'So where does that leave us?']\n",
            " > Processing time: 2.3420610427856445\n",
            " > Real-time factor: 0.09316897712637376\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/147.wav\n",
            "Generating audio for: 148.txt\n",
            " > Text splitted to sentences.\n",
            "['flow unpriced into the atmosphere any more than you should be allowed to tosh your garbage in the street.', 'We at Seeker are committed to bringing you stories that will inform and inspire you.', 'In this next episode, meet 25-year-old']\n",
            " > Processing time: 1.433933973312378\n",
            " > Real-time factor: 0.08852485136277027\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/148.wav\n",
            "Generating audio for: 149.txt\n",
            " > Text splitted to sentences.\n",
            "['4-year-old Louis Bird, an inexperienced rower who embarked on the journey of his life across the Pacific Ocean, all to connect with the memory of his father.']\n",
            " > Processing time: 1.0102579593658447\n",
            " > Real-time factor: 0.0926630116639637\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/149.wav\n",
            "Generating audio for: 150.txt\n",
            " > Text splitted to sentences.\n",
            "['Thank you for watching Seeker Daily, please make sure to like and subscribe to see new videos every day.']\n",
            " > Processing time: 0.694359302520752\n",
            " > Real-time factor: 0.0927152324180226\n",
            "Saved: /content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron/150.wav\n",
            "✅ Audio generation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code computes the PESQ score by comparing original English audio with the Tacotron-generated English audio.\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from pesq import pesq\n",
        "\n",
        "REFERENCE_AUDIO_PATH = \"/content/drive/MyDrive/selected audio 3/eng\"\n",
        "GENERATED_AUDIO_PATH = \"/content/drive/MyDrive/selected audio 3/generated audio/English/eng_tacotron\"\n",
        "\n",
        "def compute_pesq(reference_file, generated_file, sample_rate=16000):\n",
        "    ref_audio, sr1 = librosa.load(reference_file, sr=sample_rate)\n",
        "    gen_audio, sr2 = librosa.load(generated_file, sr=sample_rate)\n",
        "\n",
        "    if sr1 != sample_rate or sr2 != sample_rate:\n",
        "        ref_audio = librosa.resample(ref_audio, orig_sr=sr1, target_sr=sample_rate)\n",
        "        gen_audio = librosa.resample(gen_audio, orig_sr=sr2, target_sr=sample_rate)\n",
        "\n",
        "    min_length = min(len(ref_audio), len(gen_audio))\n",
        "    ref_audio = ref_audio[:min_length]\n",
        "    gen_audio = gen_audio[:min_length]\n",
        "\n",
        "    score = pesq(sample_rate, ref_audio, gen_audio, 'wb')\n",
        "    return score\n",
        "\n",
        "pesq_scores = {}\n",
        "for file in sorted(os.listdir(REFERENCE_AUDIO_PATH)):\n",
        "    ref_file = os.path.join(REFERENCE_AUDIO_PATH, file)\n",
        "    gen_file = os.path.join(GENERATED_AUDIO_PATH, file)\n",
        "\n",
        "    if os.path.exists(gen_file):\n",
        "        score = compute_pesq(ref_file, gen_file)\n",
        "        pesq_scores[file] = score\n",
        "        print(f\"PESQ for {file}: {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"Missing generated audio for {file}\")\n",
        "\n",
        "pesq_score_file = \"/content/drive/MyDrive/selected audio 3/generated audio/pesq score eng_tacotron_model.txt\"\n",
        "with open(pesq_score_file, \"w\") as f:\n",
        "    for file, score in pesq_scores.items():\n",
        "        f.write(f\"{file}: {score:.4f}\\n\")\n",
        "    avg_pesq = np.mean(list(pesq_scores.values()))\n",
        "    f.write(f\"\\nAverage PESQ Score: {avg_pesq:.4f}\")\n",
        "\n",
        "print(f\"\\nAverage PESQ Score: {avg_pesq:.4f}\")\n",
        "print(f\"PESQ scores saved to {pesq_score_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hxTXmCmMvT_",
        "outputId": "0d796dd3-41bf-4248-f9e4-3c64ec88e87e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PESQ for 101.wav: 1.0502\n",
            "PESQ for 102.wav: 1.1697\n",
            "PESQ for 103.wav: 1.0342\n",
            "PESQ for 104.wav: 1.0485\n",
            "PESQ for 105.wav: 1.0993\n",
            "PESQ for 106.wav: 1.0354\n",
            "PESQ for 107.wav: 1.0731\n",
            "PESQ for 108.wav: 1.0377\n",
            "PESQ for 109.wav: 1.2359\n",
            "PESQ for 110.wav: 1.0290\n",
            "PESQ for 111.wav: 1.1375\n",
            "PESQ for 112.wav: 1.2737\n",
            "PESQ for 113.wav: 1.0262\n",
            "PESQ for 114.wav: 1.0619\n",
            "PESQ for 115.wav: 1.0578\n",
            "PESQ for 116.wav: 1.4988\n",
            "PESQ for 117.wav: 1.0920\n",
            "PESQ for 118.wav: 1.0473\n",
            "PESQ for 119.wav: 1.0396\n",
            "PESQ for 120.wav: 1.0419\n",
            "PESQ for 121.wav: 1.0407\n",
            "PESQ for 122.wav: 1.0704\n",
            "PESQ for 123.wav: 1.0325\n",
            "PESQ for 124.wav: 1.0784\n",
            "PESQ for 125.wav: 1.0287\n",
            "PESQ for 126.wav: 1.1073\n",
            "PESQ for 127.wav: 1.2724\n",
            "PESQ for 128.wav: 1.1444\n",
            "PESQ for 129.wav: 1.0859\n",
            "PESQ for 130.wav: 1.2616\n",
            "PESQ for 131.wav: 1.3831\n",
            "PESQ for 132.wav: 1.4071\n",
            "PESQ for 133.wav: 1.0636\n",
            "PESQ for 134.wav: 1.0392\n",
            "PESQ for 135.wav: 1.0267\n",
            "PESQ for 136.wav: 1.1134\n",
            "PESQ for 137.wav: 1.0343\n",
            "PESQ for 138.wav: 1.0514\n",
            "PESQ for 139.wav: 1.0321\n",
            "PESQ for 140.wav: 1.0331\n",
            "PESQ for 141.wav: 1.0343\n",
            "PESQ for 142.wav: 1.0855\n",
            "PESQ for 143.wav: 1.0600\n",
            "PESQ for 144.wav: 1.1415\n",
            "PESQ for 145.wav: 1.0473\n",
            "PESQ for 146.wav: 1.0265\n",
            "PESQ for 147.wav: 1.0508\n",
            "PESQ for 148.wav: 1.0626\n",
            "PESQ for 149.wav: 1.0621\n",
            "PESQ for 150.wav: 1.1935\n",
            "\n",
            "Average PESQ Score: 1.1032\n",
            "PESQ scores saved to /content/drive/MyDrive/selected audio 3/generated audio/pesq score eng_tacotron_model.txt\n"
          ]
        }
      ]
    }
  ]
}